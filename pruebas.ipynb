{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Gsus01/NuweOracle-AnalisisDeSentimiento.git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "El nombre de archivo, el nombre de directorio o la sintaxis de la etiqueta del volumen no son correctos.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:\\Users\\3arci\\Documents\\Python\\NuweOracle-AnalisisDeSentimiento\\repo/data/train.csv' at D:\\Users\\3arci\\Documents\\Python\\NuweOracle-AnalisisDeSentimiento",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, DataCollatorWithPadding\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrepo/data/train.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m temp         \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m, data_files\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrepo/data/test.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m temp[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\load.py:1767\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   1763\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1764\u001b[0m )\n\u001b[0;32m   1766\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1767\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1768\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1769\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1770\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1771\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1772\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1773\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1774\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1775\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1776\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1777\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1778\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1779\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1780\u001b[0m )\n\u001b[0;32m   1782\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\load.py:1498\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1497\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m-> 1498\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   1499\u001b[0m     path,\n\u001b[0;32m   1500\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1501\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1502\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1503\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   1504\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   1505\u001b[0m )\n\u001b[0;32m   1507\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\load.py:1133\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[39m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[39m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \n\u001b[0;32m   1125\u001b[0m \u001b[39m# Try packaged\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m   1127\u001b[0m     \u001b[39mreturn\u001b[39;00m PackagedDatasetModuleFactory(\n\u001b[0;32m   1128\u001b[0m         path,\n\u001b[0;32m   1129\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   1130\u001b[0m         data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   1131\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1132\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m-> 1133\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[0;32m   1134\u001b[0m \u001b[39m# Try locally\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(filename):\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\load.py:708\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    704\u001b[0m base_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir)\u001b[39m.\u001b[39mresolve()) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(Path()\u001b[39m.\u001b[39mresolve())\n\u001b[0;32m    705\u001b[0m patterns \u001b[39m=\u001b[39m (\n\u001b[0;32m    706\u001b[0m     sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m get_data_patterns_locally(base_path)\n\u001b[0;32m    707\u001b[0m )\n\u001b[1;32m--> 708\u001b[0m data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39;49mfrom_local_or_remote(\n\u001b[0;32m    709\u001b[0m     patterns,\n\u001b[0;32m    710\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[0;32m    711\u001b[0m     base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[0;32m    712\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m _MODULE_SUPPORTS_METADATA \u001b[39mand\u001b[39;00m patterns \u001b[39m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n\u001b[0;32m    714\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\data_files.py:796\u001b[0m, in \u001b[0;36mDataFilesDict.from_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    793\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m()\n\u001b[0;32m    794\u001b[0m \u001b[39mfor\u001b[39;00m key, patterns_for_key \u001b[39min\u001b[39;00m patterns\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    795\u001b[0m     out[key] \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 796\u001b[0m         DataFilesList\u001b[39m.\u001b[39;49mfrom_local_or_remote(\n\u001b[0;32m    797\u001b[0m             patterns_for_key,\n\u001b[0;32m    798\u001b[0m             base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[0;32m    799\u001b[0m             allowed_extensions\u001b[39m=\u001b[39;49mallowed_extensions,\n\u001b[0;32m    800\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m    803\u001b[0m         \u001b[39melse\u001b[39;00m patterns_for_key\n\u001b[0;32m    804\u001b[0m     )\n\u001b[0;32m    805\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\data_files.py:764\u001b[0m, in \u001b[0;36mDataFilesList.from_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_local_or_remote\u001b[39m(\n\u001b[0;32m    757\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     use_auth_token: Optional[Union[\u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    762\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFilesList\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    763\u001b[0m     base_path \u001b[39m=\u001b[39m base_path \u001b[39mif\u001b[39;00m base_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(Path()\u001b[39m.\u001b[39mresolve())\n\u001b[1;32m--> 764\u001b[0m     data_files \u001b[39m=\u001b[39m resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\n\u001b[0;32m    765\u001b[0m     origin_metadata \u001b[39m=\u001b[39m _get_origin_metadata_locally_or_by_urls(data_files, use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m    766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(data_files, origin_metadata)\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\data_files.py:362\u001b[0m, in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[1;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[0;32m    360\u001b[0m         data_files\u001b[39m.\u001b[39mappend(Url(pattern))\n\u001b[0;32m    361\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 362\u001b[0m         \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):\n\u001b[0;32m    363\u001b[0m             data_files\u001b[39m.\u001b[39mappend(path)\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data_files:\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\datasets\\data_files.py:306\u001b[0m, in \u001b[0;36m_resolve_single_pattern_locally\u001b[1;34m(base_path, pattern, allowed_extensions)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[39mif\u001b[39;00m allowed_extensions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m with any supported extension \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(allowed_extensions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 306\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[0;32m    307\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(out)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to find 'D:\\Users\\3arci\\Documents\\Python\\NuweOracle-AnalisisDeSentimiento\\repo/data/train.csv' at D:\\Users\\3arci\\Documents\\Python\\NuweOracle-AnalisisDeSentimiento"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"repo/data/train.csv\")\n",
    "temp         = load_dataset(\"csv\", data_files=\"repo/data/test.csv\")\n",
    "raw_datasets[\"test\"] = temp[\"train\"]\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "  return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "  columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "  label_cols=[\"label\"],\n",
    "  shuffle=True,\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=8,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "  columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "  shuffle=False,\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=\"adam\",\n",
    "#   loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "#   metrics=[\"accuracy\"],\n",
    "# )\n",
    "# model.fit(\n",
    "#   tf_train_dataset,\n",
    "#   # validation_data=tf_validation_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "  initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named tf_model.h5 or pytorch_model.bin found in directory model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TFAutoModelForSequenceClassification\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m TFAutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\3arci\\miniconda3\\envs\\oracle\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2630\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2625\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError no file named \u001b[39m\u001b[39m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m found in directory \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2626\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbut there is a file for PyTorch weights. Use `from_pt=True` to load this model from those \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2627\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mweights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m         )\n\u001b[0;32m   2629\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2630\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2631\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError no file named \u001b[39m\u001b[39m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mWEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m found in directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2632\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2633\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(pretrained_model_name_or_path):\n\u001b[0;32m   2635\u001b[0m     archive_file \u001b[39m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[1;31mOSError\u001b[0m: Error no file named tf_model.h5 or pytorch_model.bin found in directory model."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tf_test_dataset)[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds = np.argmax(preds, axis=1)\n",
    "print(preds.shape, class_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {}\n",
    "for idx, pred in enumerate(preds):\n",
    "  target[str(idx)] = 0 if pred[0] > pred[1] else 1\n",
    "predictions = { 'target': target }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('predictions.json', 'w') as f:\n",
    "  json.dump(predictions, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
